{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Pytorch code for what the paper's model might look like.\n",
    "A quick tutorial on Pytorch can be found at https://cs230-stanford.github.io/pytorch-getting-started.html\n",
    "There's also the Pytorch documentation intro at https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "which goes into a few more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Modified LSTM modal\n",
    "This contains both the embedding layers, Pytorch's default LSTM layer,\n",
    "and 2 linear layers\n",
    "'''\n",
    "class PathLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, e_emb_dim, t_emb_dim, r_emb_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(PathLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.entity_embeddings = nn.Embedding(vocab_size, e_emb_dim)\n",
    "        self.type_embeddings = nn.Embedding(vocab_size, t_emb_dim)\n",
    "        self.rel_embeddings = nn.Embedding(vocab_size, r_emb_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(e_emb_dim + t_emb_dim + r_emb_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to to tags\n",
    "        self.linear1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, paths):      \n",
    "        #transpose, so entities 1st row, types 2nd row, and relations 3nd (these are dim 1 and 2 since batch is 0)\n",
    "        #this could just be the input if we want\n",
    "        t_paths = torch.transpose(paths, 1, 2)\n",
    "        \n",
    "        #then concatenate embeddings, batch is index 0, so selecting along index 1\n",
    "        entity_embed = self.entity_embeddings(t_paths[:,0,:])\n",
    "        type_embed = self.type_embeddings(t_paths[:,1,:])\n",
    "        rel_embed = self.rel_embeddings(t_paths[:,2,:])\n",
    "        triplet_embed = torch.cat((entity_embed, type_embed, rel_embed), 2) #concatenates lengthwise\n",
    "        \n",
    "        #we need dimensions to be input size x batches x embedding dim\n",
    "        #currently assuming all paths are same length\n",
    "        lstm_out, last_state = self.lstm(triplet_embed.view(len(paths[0]), len(paths), -1))\n",
    "        \n",
    "        # Retrieve the final hidden state info, [-1,:,:], since we want output from last input item\n",
    "        tag_score = self.linear2(F.relu(self.linear1(lstm_out[-1,:,:])))\n",
    "        \n",
    "        #Paper uses relu as final activation, but for Pytorch's nllloss it seems like we need a softmax layer\n",
    "        #to convert to probability distribution?\n",
    "        #return F.relu(tag_score)\n",
    "        return F.log_softmax(tag_score, dim=1)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sam', 'u', 'rate'], ['Song1', 's', '_rate'], ['Joey', 'u', 'rate'], ['Song3', 's', 'UNK']]\n",
      "tensor([[10,  7,  0],\n",
      "        [14,  8,  1],\n",
      "        [17,  9,  5],\n",
      "        [15,  8,  6]])\n",
      "[(tensor([[10,  7,  0],\n",
      "        [14,  8,  1],\n",
      "        [17,  9,  5],\n",
      "        [15,  8,  6]]), 1), (tensor([[10,  7,  0],\n",
      "        [14,  8,  3],\n",
      "        [13,  7,  0],\n",
      "        [16,  8,  6]]), 0)]\n"
     ]
    }
   ],
   "source": [
    "#For now just construct example, later would want to automatically create maps from vocab\n",
    "e_to_ix = {'Sam': 0, 'Weijia': 1, 'Rosa': 2, 'Joey':3, 'Song1': 4, 'Song2': 5, 'Song3': 6, 'Pop': 7}\n",
    "t_to_ix = {'u': 0, 's': 1, 't': 2}\n",
    "r_to_ix = {'rate': 0, 'category': 1, 'belong': 2, '_rate': 3, '_category': 4, '_belong':5, 'UNK': 6}\n",
    "\n",
    "#TODO: decide on actual path data format\n",
    "#this could be transposed to [[entity1, entity2, ...], [type1, type2, ...], [rel1, rel2, ...]]\n",
    "#since we do that in the model\n",
    "training_data = [\n",
    "    ([['Sam', 'u', 'rate'], ['Song1', 's', 'category'], ['Pop', 't', '_belong'], ['Song2', 's', 'UNK']], 1),\n",
    "    ([['Sam', 'u', 'rate'], ['Song1', 's', '_rate'], ['Joey', 'u', 'rate'],['Song3', 's', 'UNK']], 0)\n",
    "]\n",
    "\n",
    "#construct tensor of item, type, and relation ids\n",
    "def prepare_path(seq, e_to_ix, t_to_ix, r_to_ix):\n",
    "    id_pairs = []\n",
    "    for step in seq:\n",
    "        e,t,r = step[0], step[1], step[2]\n",
    "        id_pairs.append([len(t_to_ix) + len(r_to_ix) + e_to_ix[e], len(r_to_ix) + t_to_ix[t], r_to_ix[r]])\n",
    "    \n",
    "    return torch.tensor(id_pairs, dtype=torch.long)\n",
    "\n",
    "print(training_data[1][0])\n",
    "print(prepare_path(training_data[0][0], e_to_ix, t_to_ix, r_to_ix))\n",
    "\n",
    "formatted_data = []\n",
    "for path, tag in training_data:\n",
    "    formatted_data.append((prepare_path(path, e_to_ix, t_to_ix, r_to_ix), tag))\n",
    "print(formatted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is: 0.7014683485031128\n",
      "loss is: 0.7388348579406738\n",
      "loss is: 0.5947420597076416\n",
      "loss is: 0.695090115070343\n",
      "loss is: 0.5296273827552795\n",
      "loss is: 0.6481267213821411\n",
      "loss is: 0.47548192739486694\n",
      "loss is: 0.6101570129394531\n",
      "loss is: 0.4242744445800781\n",
      "loss is: 0.5779571533203125\n",
      "loss is: 0.36931928992271423\n",
      "loss is: 0.539128839969635\n",
      "loss is: 0.3117123246192932\n",
      "loss is: 0.5211480259895325\n",
      "loss is: 0.25671860575675964\n",
      "loss is: 0.5027366876602173\n",
      "loss is: 0.20797215402126312\n",
      "loss is: 0.481269508600235\n",
      "loss is: 0.16375774145126343\n",
      "loss is: 0.4568486511707306\n",
      "loss is: 0.12568672001361847\n",
      "loss is: 0.4299483001232147\n",
      "loss is: 0.09451020509004593\n",
      "loss is: 0.40103062987327576\n",
      "loss is: 0.0701766312122345\n",
      "loss is: 0.37055596709251404\n",
      "loss is: 0.05197293311357498\n",
      "loss is: 0.339013934135437\n",
      "loss is: 0.038773197680711746\n",
      "loss is: 0.30698487162590027\n",
      "loss is: 0.029356908053159714\n",
      "loss is: 0.27517032623291016\n",
      "loss is: 0.022656399756669998\n",
      "loss is: 0.2443258911371231\n",
      "loss is: 0.017853859812021255\n",
      "loss is: 0.21512897312641144\n",
      "loss is: 0.014368640258908272\n",
      "loss is: 0.18808382749557495\n",
      "loss is: 0.011802486144006252\n",
      "loss is: 0.16350126266479492\n",
      "loss is: 0.009884695522487164\n",
      "loss is: 0.1415204256772995\n",
      "loss is: 0.008430605754256248\n",
      "loss is: 0.1221417784690857\n",
      "loss is: 0.007313266396522522\n",
      "loss is: 0.10526217520236969\n",
      "loss is: 0.006443439517170191\n",
      "loss is: 0.0907067060470581\n",
      "loss is: 0.005758363753557205\n",
      "loss is: 0.07825730741024017\n",
      "loss is: 0.005212763790041208\n",
      "loss is: 0.06767618656158447\n",
      "loss is: 0.004773648921400309\n",
      "loss is: 0.05872388184070587\n",
      "loss is: 0.004416948650032282\n",
      "loss is: 0.05117214843630791\n",
      "loss is: 0.00412446865811944\n",
      "loss is: 0.04481140896677971\n",
      "loss is: 0.0038826095405966043\n",
      "loss is: 0.039455171674489975\n",
      "loss is: 0.0036810750607401133\n",
      "loss is: 0.03494097292423248\n",
      "loss is: 0.003511692862957716\n",
      "loss is: 0.031129714101552963\n",
      "loss is: 0.0033684202935546637\n",
      "loss is: 0.027904100716114044\n",
      "loss is: 0.0032463965471833944\n",
      "loss is: 0.025165708735585213\n",
      "loss is: 0.003141945693641901\n",
      "loss is: 0.022832823917269707\n",
      "loss is: 0.00305174570530653\n",
      "loss is: 0.020837673917412758\n",
      "loss is: 0.0029735418502241373\n",
      "loss is: 0.019124522805213928\n",
      "loss is: 0.002905316650867462\n",
      "loss is: 0.017647039145231247\n",
      "loss is: 0.0028455264400690794\n",
      "loss is: 0.016367461532354355\n",
      "loss is: 0.002792865503579378\n",
      "loss is: 0.015254145488142967\n",
      "loss is: 0.0027461457066237926\n",
      "loss is: 0.01428120955824852\n",
      "loss is: 0.0027046550530940294\n",
      "loss is: 0.0134271876886487\n",
      "loss is: 0.002667323686182499\n",
      "loss is: 0.012674258090555668\n",
      "loss is: 0.0026337956078350544\n",
      "loss is: 0.0120074562728405\n",
      "loss is: 0.002603476867079735\n",
      "loss is: 0.011414343491196632\n",
      "loss is: 0.002575772814452648\n",
      "loss is: 0.010884556919336319\n",
      "loss is: 0.002550446195527911\n",
      "loss is: 0.010409348644316196\n",
      "loss is: 0.0025270215701311827\n",
      "loss is: 0.009981244802474976\n",
      "loss is: 0.0025053799618035555\n",
      "loss is: 0.009594164788722992\n",
      "loss is: 0.0024850459303706884\n",
      "loss is: 0.009242723695933819\n",
      "loss is: 0.002465900732204318\n",
      "loss is: 0.008922350592911243\n",
      "loss is: 0.0024479443673044443\n",
      "loss is: 0.00862929504364729\n",
      "loss is: 0.0024307011626660824\n",
      "loss is: 0.008360151201486588\n",
      "loss is: 0.0024142900947481394\n",
      "loss is: 0.008112220093607903\n",
      "loss is: 0.002398592187091708\n",
      "loss is: 0.007882914505898952\n",
      "loss is: 0.002383369952440262\n",
      "loss is: 0.007670118007808924\n",
      "loss is: 0.002368622925132513\n",
      "loss is: 0.007472185418009758\n",
      "loss is: 0.002354232594370842\n",
      "loss is: 0.0072873495519161224\n",
      "loss is: 0.0023403179366141558\n",
      "loss is: 0.0071143158711493015\n",
      "loss is: 0.0023266407661139965\n",
      "loss is: 0.006951788440346718\n",
      "loss is: 0.002313439268618822\n",
      "loss is: 0.006798943970352411\n",
      "loss is: 0.002300356514751911\n",
      "loss is: 0.006654602009803057\n",
      "loss is: 0.0022876302246004343\n",
      "loss is: 0.006518056150525808\n",
      "loss is: 0.0022751418873667717\n",
      "loss is: 0.0063887168653309345\n",
      "loss is: 0.002262891037389636\n",
      "loss is: 0.006265758071094751\n",
      "loss is: 0.0022509971167892218\n",
      "loss is: 0.006148708052933216\n",
      "loss is: 0.0022392217069864273\n",
      "loss is: 0.006037095095962286\n",
      "loss is: 0.002227684250101447\n",
      "loss is: 0.005930565297603607\n",
      "loss is: 0.0022162655368447304\n",
      "loss is: 0.005828527733683586\n",
      "loss is: 0.0022052035201340914\n",
      "loss is: 0.005730865523219109\n",
      "loss is: 0.0021942604798823595\n",
      "loss is: 0.00563710555434227\n",
      "loss is: 0.0021835551597177982\n",
      "loss is: 0.005547130014747381\n",
      "loss is: 0.0021729685831815004\n",
      "loss is: 0.005460585001856089\n",
      "loss is: 0.002162500750273466\n",
      "loss is: 0.005377352237701416\n",
      "loss is: 0.002152270870283246\n",
      "loss is: 0.005297077354043722\n",
      "loss is: 0.0021421597339212894\n",
      "loss is: 0.005219760350883007\n",
      "loss is: 0.00213216757401824\n",
      "loss is: 0.005145165137946606\n",
      "loss is: 0.0021222943905740976\n",
      "loss is: 0.005073055624961853\n",
      "loss is: 0.002112539717927575\n",
      "loss is: 0.005003431346267462\n",
      "loss is: 0.0021029042545706034\n",
      "loss is: 0.004936174489557743\n",
      "loss is: 0.002093268558382988\n",
      "loss is: 0.004871048964560032\n",
      "loss is: 0.002083870582282543\n",
      "loss is: 0.004808054305613041\n",
      "loss is: 0.0020743536297231913\n",
      "loss is: 0.004747072700411081\n",
      "loss is: 0.0020650746300816536\n",
      "loss is: 0.004687985870987177\n",
      "loss is: 0.002055676421150565\n",
      "loss is: 0.0046306755393743515\n",
      "loss is: 0.002046516165137291\n",
      "loss is: 0.004575022961944342\n",
      "loss is: 0.002037355676293373\n",
      "loss is: 0.004521029070019722\n",
      "loss is: 0.0020281951874494553\n",
      "loss is: 0.004468574654310942\n",
      "loss is: 0.0020191536750644445\n",
      "loss is: 0.004417541902512312\n",
      "loss is: 0.00201011192984879\n",
      "loss is: 0.004367930814623833\n",
      "loss is: 0.002001189161092043\n",
      "loss is: 0.004319623112678528\n",
      "loss is: 0.001992385368794203\n",
      "loss is: 0.004272618796676397\n",
      "loss is: 0.001983581343665719\n",
      "loss is: 0.004226680379360914\n",
      "loss is: 0.0019748962949961424\n",
      "loss is: 0.004182045813649893\n",
      "loss is: 0.001966211013495922\n",
      "loss is: 0.004138358868658543\n",
      "loss is: 0.0019576449412852526\n",
      "loss is: 0.004095738288015127\n",
      "loss is: 0.0019490785198286176\n",
      "loss is: 0.004054185003042221\n",
      "loss is: 0.0019406310748308897\n",
      "loss is: 0.004013460595160723\n",
      "loss is: 0.001932302606292069\n",
      "loss is: 0.00397368473932147\n",
      "loss is: 0.0019239740213379264\n",
      "loss is: 0.003934738226234913\n",
      "loss is: 0.001915764412842691\n",
      "loss is: 0.003896740498021245\n",
      "loss is: 0.0019075546879321337\n",
      "loss is: 0.0038594536017626524\n",
      "loss is: 0.0018994638230651617\n",
      "loss is: 0.003822996746748686\n",
      "loss is: 0.0018913729581981897\n",
      "loss is: 0.003787250956520438\n",
      "loss is: 0.001883400953374803\n",
      "loss is: 0.003752216463908553\n",
      "loss is: 0.0018754289485514164\n",
      "loss is: 0.0037180122453719378\n",
      "loss is: 0.001867456827312708\n",
      "loss is: 0.0036844005808234215\n",
      "loss is: 0.0018596036825329065\n",
      "loss is: 0.003651500679552555\n",
      "loss is: 0.0018517505377531052\n",
      "loss is: 0.003619193332269788\n",
      "loss is: 0.0018438971601426601\n",
      "loss is: 0.0035875977482646704\n",
      "loss is: 0.001836162875406444\n",
      "loss is: 0.0035564762074500322\n",
      "loss is: 0.0018285474507138133\n",
      "loss is: 0.0035260666627436876\n",
      "loss is: 0.0018208129331469536\n",
      "loss is: 0.0034962499048560858\n",
      "loss is: 0.001813197392039001\n",
      "loss is: 0.003467026399448514\n",
      "loss is: 0.0018055817345157266\n",
      "loss is: 0.003438277170062065\n",
      "loss is: 0.001798085169866681\n",
      "loss is: 0.0034101211931556463\n",
      "loss is: 0.001790588372386992\n",
      "loss is: 0.003382558235898614\n",
      "loss is: 0.0017832106677815318\n",
      "loss is: 0.0033553512766957283\n",
      "loss is: 0.001775832730345428\n",
      "loss is: 0.003328737337142229\n",
      "loss is: 0.0017685738857835531\n",
      "loss is: 0.00330259813927114\n",
      "loss is: 0.0017613149248063564\n",
      "loss is: 0.0032769334502518177\n",
      "loss is: 0.0017540559638291597\n",
      "loss is: 0.0032517435029149055\n",
      "loss is: 0.0017470349557697773\n",
      "loss is: 0.0032269093208014965\n",
      "loss is: 0.001740013831295073\n",
      "loss is: 0.003202668856829405\n",
      "loss is: 0.0017329927068203688\n",
      "loss is: 0.003178665181621909\n",
      "loss is: 0.0017260904423892498\n",
      "loss is: 0.003155255224555731\n",
      "loss is: 0.0017193072708323598\n",
      "loss is: 0.0031322012655436993\n",
      "loss is: 0.001712523982860148\n",
      "loss is: 0.003109503071755171\n",
      "loss is: 0.0017058596713468432\n",
      "loss is: 0.003087161108851433\n",
      "loss is: 0.0016993143362924457\n",
      "loss is: 0.0030651751440018415\n",
      "loss is: 0.0016927688848227262\n",
      "loss is: 0.003043664153665304\n",
      "loss is: 0.001686342409811914\n",
      "loss is: 0.0030225091613829136\n",
      "loss is: 0.0016799159348011017\n",
      "loss is: 0.0030015914235264063\n",
      "loss is: 0.0016736084362491965\n",
      "loss is: 0.002981148660182953\n",
      "loss is: 0.0016674199141561985\n",
      "loss is: 0.0029609431512653828\n",
      "loss is: 0.0016612313920632005\n",
      "loss is: 0.002941093873232603\n",
      "loss is: 0.0016551617300137877\n",
      "loss is: 0.002921600826084614\n",
      "loss is: 0.001649211160838604\n",
      "loss is: 0.002902345033362508\n",
      "loss is: 0.0016432604752480984\n",
      "loss is: 0.0028834454715251923\n",
      "loss is: 0.0016374287661165\n",
      "loss is: 0.002864902140572667\n",
      "loss is: 0.0016317160334438086\n",
      "loss is: 0.002846596296876669\n",
      "loss is: 0.0016260033007711172\n",
      "loss is: 0.0028285279404371977\n",
      "loss is: 0.001620409544557333\n",
      "loss is: 0.002810815814882517\n",
      "loss is: 0.0016148157883435488\n",
      "loss is: 0.0027933409437537193\n",
      "loss is: 0.0016093410085886717\n",
      "loss is: 0.0027761037927120924\n",
      "loss is: 0.0016039852052927017\n",
      "loss is: 0.0027592226397246122\n",
      "loss is: 0.0015986294019967318\n",
      "loss is: 0.0027425792068243027\n",
      "loss is: 0.001593392575159669\n",
      "loss is: 0.0027260545175522566\n",
      "loss is: 0.0015881556319072843\n",
      "loss is: 0.002709886059165001\n",
      "loss is: 0.0015830377815291286\n",
      "loss is: 0.002693955088034272\n",
      "loss is: 0.00157803890760988\n",
      "loss is: 0.0026782616041600704\n",
      "loss is: 0.0015730400336906314\n",
      "loss is: 0.0026628058403730392\n",
      "loss is: 0.00156816013623029\n",
      "loss is: 0.0026475873310118914\n",
      "loss is: 0.0015632801223546267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is: 0.0026326067745685577\n",
      "loss is: 0.0015585192013531923\n",
      "loss is: 0.0026177444960922003\n",
      "loss is: 0.001553758280351758\n",
      "loss is: 0.002603120170533657\n",
      "loss is: 0.0015491163358092308\n",
      "loss is: 0.0025888520758599043\n",
      "loss is: 0.0015444743912667036\n",
      "loss is: 0.002574583748355508\n",
      "loss is: 0.0015399513067677617\n",
      "loss is: 0.002560672117397189\n",
      "loss is: 0.0015355474315583706\n",
      "loss is: 0.0025468789972364902\n",
      "loss is: 0.0015311434399336576\n",
      "loss is: 0.002533323597162962\n",
      "loss is: 0.0015267394483089447\n",
      "loss is: 0.002519886940717697\n",
      "loss is: 0.0015224544331431389\n",
      "loss is: 0.002506688004359603\n",
      "loss is: 0.0015182883944362402\n",
      "loss is: 0.0024937265552580357\n",
      "loss is: 0.0015140033792704344\n",
      "loss is: 0.0024808840826153755\n",
      "loss is: 0.0015099564334377646\n",
      "loss is: 0.0024682790972292423\n",
      "loss is: 0.001505909371189773\n",
      "loss is: 0.0024557928554713726\n",
      "loss is: 0.0015018623089417815\n",
      "loss is: 0.0024434253573417664\n",
      "loss is: 0.0014978153631091118\n",
      "loss is: 0.0024312958121299744\n",
      "loss is: 0.0014940063701942563\n",
      "loss is: 0.002419284777715802\n",
      "loss is: 0.0014900782844051719\n",
      "loss is: 0.0024075114633888006\n",
      "loss is: 0.0014862692914903164\n",
      "loss is: 0.0023958568926900625\n",
      "loss is: 0.001482579275034368\n",
      "loss is: 0.0023843212984502316\n",
      "loss is: 0.0014788892585784197\n",
      "loss is: 0.0023730231914669275\n",
      "loss is: 0.0014751992421224713\n",
      "loss is: 0.0023617250844836235\n",
      "loss is: 0.00147162820212543\n",
      "loss is: 0.0023506649304181337\n",
      "loss is: 0.0014680571621283889\n",
      "loss is: 0.002339842263609171\n",
      "loss is: 0.0014644861221313477\n",
      "loss is: 0.0023290193639695644\n",
      "loss is: 0.0014610340585932136\n",
      "loss is: 0.002318434417247772\n",
      "loss is: 0.0014575821114704013\n",
      "loss is: 0.0023079682141542435\n",
      "loss is: 0.0014542490243911743\n",
      "loss is: 0.0022975020110607147\n",
      "loss is: 0.0014509160537272692\n",
      "loss is: 0.00228739227168262\n",
      "loss is: 0.001447583083063364\n",
      "loss is: 0.002277282765135169\n",
      "loss is: 0.001444369088858366\n",
      "loss is: 0.002267291769385338\n",
      "loss is: 0.0014411549782380462\n",
      "loss is: 0.0022574197500944138\n",
      "loss is: 0.0014379409840330482\n",
      "loss is: 0.002247666707262397\n",
      "loss is: 0.0014348459662869573\n",
      "loss is: 0.0022381513845175505\n",
      "loss is: 0.0014317509485408664\n",
      "loss is: 0.0022286358289420605\n",
      "loss is: 0.0014287750236690044\n",
      "loss is: 0.0022193582262843847\n",
      "loss is: 0.0014256800059229136\n",
      "loss is: 0.0022100803907960653\n",
      "loss is: 0.0014227039646357298\n",
      "loss is: 0.002200921531766653\n",
      "loss is: 0.001419847016222775\n",
      "loss is: 0.0021918814163655043\n",
      "loss is: 0.001416871091350913\n",
      "loss is: 0.0021830792538821697\n",
      "loss is: 0.0014140140265226364\n",
      "loss is: 0.0021742768585681915\n",
      "loss is: 0.0014112761709839106\n",
      "loss is: 0.00216821045614779\n",
      "loss is: 0.0014148473273962736\n",
      "loss is: 0.0021333571057766676\n",
      "loss is: 0.0014222278259694576\n",
      "loss is: 0.0021084952168166637\n",
      "loss is: 0.0014252038672566414\n",
      "loss is: 0.002090770285576582\n",
      "loss is: 0.001423656358383596\n",
      "loss is: 0.002079825848340988\n",
      "loss is: 0.0014179424615576863\n",
      "loss is: 0.0020743536297231913\n",
      "loss is: 0.001409133430570364\n",
      "loss is: 0.002078041434288025\n",
      "loss is: 0.001405086019076407\n",
      "loss is: 0.002052107360213995\n",
      "loss is: 0.0014040146488696337\n",
      "loss is: 0.0020387833938002586\n",
      "loss is: 0.0013992529129609466\n",
      "loss is: 0.0020325970835983753\n",
      "loss is: 0.0013979434734210372\n",
      "loss is: 0.0020086844451725483\n",
      "loss is: 0.0013981815427541733\n",
      "loss is: 0.001995002618059516\n",
      "loss is: 0.0013935388997197151\n",
      "loss is: 0.001989648910239339\n",
      "loss is: 0.0013849677052348852\n",
      "loss is: 0.001990719698369503\n",
      "loss is: 0.0013740155845880508\n",
      "loss is: 0.0019997614435851574\n",
      "loss is: 0.0013685394078493118\n",
      "loss is: 0.001981796696782112\n",
      "loss is: 0.0013672299683094025\n",
      "loss is: 0.0019740634597837925\n",
      "loss is: 0.0013633014168590307\n",
      "loss is: 0.0019748962949961424\n",
      "loss is: 0.0013637775555253029\n",
      "loss is: 0.0019516960019245744\n",
      "loss is: 0.0013667537132278085\n",
      "loss is: 0.0019402741454541683\n",
      "loss is: 0.0013653251808136702\n",
      "loss is: 0.001936228945851326\n",
      "loss is: 0.0013600870734080672\n",
      "loss is: 0.0019455092260614038\n",
      "loss is: 0.0013678251998499036\n",
      "loss is: 0.001897679059766233\n",
      "loss is: 0.001383658149279654\n",
      "loss is: 0.0018732872558757663\n",
      "loss is: 0.00138853897806257\n",
      "loss is: 0.0018629353726282716\n",
      "loss is: 0.0013831820106133819\n",
      "loss is: 0.001864363206550479\n",
      "loss is: 0.001369848963804543\n",
      "loss is: 0.0018745961133390665\n",
      "loss is: 0.0013517538318410516\n",
      "loss is: 0.0018913729581981897\n",
      "loss is: 0.001341634662821889\n",
      "loss is: 0.001884471857920289\n",
      "loss is: 0.0013438966125249863\n",
      "loss is: 0.001863292302004993\n",
      "loss is: 0.0013466347008943558\n",
      "loss is: 0.0018529404187574983\n",
      "loss is: 0.0013431822881102562\n",
      "loss is: 0.0018516314448788762\n",
      "loss is: 0.001334729720838368\n",
      "loss is: 0.0018596036825329065\n",
      "loss is: 0.0013293724041432142\n",
      "loss is: 0.0018463960150256753\n",
      "loss is: 0.0013325868640094995\n",
      "loss is: 0.0018225978128612041\n",
      "loss is: 0.001335444045253098\n",
      "loss is: 0.0018112935358658433\n",
      "loss is: 0.0013316344702616334\n",
      "loss is: 0.0018105795606970787\n",
      "loss is: 0.0013316344702616334\n",
      "loss is: 0.0017940392717719078\n",
      "loss is: 0.0013335392577573657\n",
      "loss is: 0.0017896364443004131\n",
      "loss is: 0.001328062848187983\n",
      "loss is: 0.0017949911998584867\n",
      "loss is: 0.0013169910525903106\n",
      "loss is: 0.001819028053432703\n",
      "loss is: 0.0013187768636271358\n",
      "loss is: 0.0017779747722670436\n",
      "loss is: 0.001328896265476942\n",
      "loss is: 0.0017629809444770217\n",
      "loss is: 0.0013293724041432142\n",
      "loss is: 0.0017604819731786847\n",
      "loss is: 0.001321634161286056\n",
      "loss is: 0.0017677409341558814\n",
      "loss is: 0.0013080621138215065\n",
      "loss is: 0.0017909454181790352\n",
      "loss is: 0.0012980615720152855\n",
      "loss is: 0.0017940392717719078\n",
      "loss is: 0.0013071097200736403\n",
      "loss is: 0.0017364437226206064\n",
      "loss is: 0.0013231817865744233\n",
      "loss is: 0.001712642959319055\n",
      "loss is: 0.0013272295473143458\n",
      "loss is: 0.0017049076268449426\n",
      "loss is: 0.0013198483502492309\n",
      "loss is: 0.0017105009173974395\n",
      "loss is: 0.0013043713988736272\n",
      "loss is: 0.0017259714659303427\n",
      "loss is: 0.0012846082681789994\n",
      "loss is: 0.0017644088948145509\n",
      "loss is: 0.001279965159483254\n",
      "loss is: 0.001727637485601008\n",
      "loss is: 0.001292465953156352\n",
      "loss is: 0.0016952680889517069\n",
      "loss is: 0.0013016331940889359\n",
      "loss is: 0.0016817011637613177\n",
      "loss is: 0.0013004426145926118\n",
      "loss is: 0.001681939116679132\n",
      "loss is: 0.001290561049245298\n",
      "loss is: 0.0016931259306147695\n",
      "loss is: 0.0012753218179568648\n",
      "loss is: 0.0017215682892128825\n",
      "loss is: 0.0012736550997942686\n",
      "loss is: 0.0016890796832740307\n",
      "loss is: 0.0012811556225642562\n",
      "loss is: 0.0016795588890090585\n",
      "loss is: 0.001280679483897984\n",
      "loss is: 0.0016815820708870888\n",
      "loss is: 0.0012734169140458107\n",
      "loss is: 0.001699671265669167\n",
      "loss is: 0.0012679402716457844\n",
      "loss is: 0.0016989572905004025\n",
      "loss is: 0.001279846066609025\n",
      "loss is: 0.001647664001211524\n",
      "loss is: 0.0012969900853931904\n",
      "loss is: 0.0016269554616883397\n",
      "loss is: 0.0013003236381337047\n",
      "loss is: 0.0016232660273090005\n",
      "loss is: 0.001291394466534257\n",
      "loss is: 0.0016332633094862103\n",
      "loss is: 0.0012738931691274047\n",
      "loss is: 0.0016540905926376581\n",
      "loss is: 0.001258296542800963\n",
      "loss is: 0.0016684910515323281\n",
      "loss is: 0.0012613920262083411\n",
      "loss is: 0.0016277885297313333\n",
      "loss is: 0.0012718691723421216\n",
      "loss is: 0.0016132686287164688\n",
      "loss is: 0.0012716311030089855\n",
      "loss is: 0.001613506581634283\n",
      "loss is: 0.001262463629245758\n",
      "loss is: 0.0016251702327281237\n",
      "loss is: 0.0012475810945034027\n",
      "loss is: 0.0016592082101851702\n",
      "loss is: 0.0012460333527997136\n",
      "loss is: 0.0016230279579758644\n",
      "loss is: 0.0012538912706077099\n",
      "loss is: 0.0016148157883435488\n",
      "loss is: 0.0012532960390672088\n",
      "loss is: 0.001618624315597117\n",
      "loss is: 0.0012521054595708847\n",
      "loss is: 0.0016110072610899806\n",
      "loss is: 0.0012534151319414377\n",
      "loss is: 0.0016480210470035672\n",
      "loss is: 0.001209600013680756\n",
      "loss is: 0.0016229089815169573\n",
      "loss is: 0.001189954113215208\n",
      "loss is: 0.0016224328428506851\n",
      "loss is: 0.0011739989276975393\n",
      "loss is: 0.0016282646683976054\n",
      "loss is: 0.0011681645410135388\n",
      "loss is: 0.0016156489728018641\n",
      "loss is: 0.0011701886542141438\n",
      "loss is: 0.0016110072610899806\n",
      "loss is: 0.0011731653939932585\n",
      "loss is: 0.001615410903468728\n",
      "loss is: 0.0011853104224428535\n",
      "loss is: 0.0015938685974106193\n",
      "loss is: 0.0012031705118715763\n",
      "loss is: 0.0015861323336139321\n",
      "loss is: 0.001215077005326748\n",
      "loss is: 0.0015902980230748653\n",
      "loss is: 0.0012267453130334616\n",
      "loss is: 0.0015797051601111889\n",
      "loss is: 0.0012559153838083148\n",
      "loss is: 0.0015773248160257936\n",
      "loss is: 0.0012180536286905408\n",
      "loss is: 0.0015555436257272959\n",
      "loss is: 0.0012070996453985572\n",
      "loss is: 0.0015486401971429586\n",
      "loss is: 0.0011937642702832818\n",
      "loss is: 0.0015529250958934426\n",
      "loss is: 0.0011798333143815398\n",
      "loss is: 0.0015654225135222077\n",
      "loss is: 0.001167331007309258\n",
      "loss is: 0.00158565619494766\n",
      "loss is: 0.00116637849714607\n",
      "loss is: 0.0015768486773595214\n",
      "loss is: 0.0011743560899049044\n",
      "loss is: 0.0015812524361535907\n",
      "loss is: 0.0011863820254802704\n",
      "loss is: 0.0015636371681466699\n",
      "loss is: 0.0012057899730280042\n",
      "loss is: 0.0015387610765174031\n",
      "loss is: 0.0012415089877322316\n",
      "loss is: 0.001545426552183926\n",
      "loss is: 0.0011815002653747797\n",
      "loss is: 0.0016001766780391335\n",
      "loss is: 0.0011631635716184974\n",
      "loss is: 0.001555900671519339\n",
      "loss is: 0.0011712603736668825\n",
      "loss is: 0.0015198357868939638\n",
      "loss is: 0.0011847150744870305\n",
      "loss is: 0.0015013862866908312\n",
      "loss is: 0.001192097319290042\n",
      "loss is: 0.0014972201315686107\n"
     ]
    }
   ],
   "source": [
    "E_EMBEDDING_DIM = 6 #64 in paper\n",
    "T_EMBEDDING_DIM = 6 #32 in paper\n",
    "R_EMBEDDING_DIM = 6 #32 in paper\n",
    "HIDDEN_DIM = 6 #this might be unit number = 256\n",
    "TARGET_SIZE = 2\n",
    "\n",
    "vocab_size = len(e_to_ix) + len(t_to_ix) + len(r_to_ix)\n",
    "model = PathLSTM(E_EMBEDDING_DIM, T_EMBEDDING_DIM, R_EMBEDDING_DIM, HIDDEN_DIM, vocab_size, TARGET_SIZE)\n",
    "loss_function = nn.NLLLoss() #negative log likelihood loss\n",
    "#loss_function = nn.CrossEntropyLoss() #This seems to work with relu activation but nllloss does not\n",
    "#this is because crossEntropyLoss actually automatically adds the softmax layer to normalize results into p-distribution\n",
    "\n",
    "\n",
    "# l2 regularization is tuned from {10−5 , 10−4 , 10−3 , 10−2 }, I think this is weight decay\n",
    "# Learning rate is found from {0.001, 0.002, 0.01, 0.02} with grid search\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=.001)\n",
    "\n",
    "#DataLoader used for batches\n",
    "train_loader = DataLoader(dataset=formatted_data, batch_size=1, shuffle=False)\n",
    "\n",
    "for epoch in range(300):  # tiny data so 300 epochs\n",
    "    for path_batch, target_batch in train_loader:   \n",
    "        #Pytorch accumulates gradients, so we need to clear before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        #Run the forward pass.\n",
    "        tag_scores = model(path_batch)\n",
    "\n",
    "        #Compute the loss, gradients, and update the parameters by calling .step()\n",
    "        loss = loss_function(tag_scores, target_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        print(\"loss is:\", loss.item())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-6.7319e+00, -1.1930e-03]])\n",
      "tensor([[-1.4863e-03, -6.5122e+00]])\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are after training, on the training dataset\n",
    "with torch.no_grad():\n",
    "    test_loader = DataLoader(dataset=formatted_data, batch_size=2, shuffle=False)\n",
    "    for path_batch, target_batch in train_loader:\n",
    "        tag_scores = model(path_batch)\n",
    "        print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
